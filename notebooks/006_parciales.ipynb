{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-01 Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una red social almacena el contenido de los chats entre sus usuarios\n",
    "en un RDD que tiene registros con el siguiente formato: (chat_id,\n",
    "user_id, nickname, text). Queremos saber cuál es el usuario (user_id)\n",
    "que mas preguntas hace en sus chats, contabilizamos una pregunta por\n",
    "cada caracter “?” que aparezca en el campo text. Programar en Spark\n",
    "un programa que identifique al usuario preguntón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chats = [\n",
    "    (1, 1, 'damu', 'Qué es esto?'),\n",
    "    (2, 2, 'martin', 'Un chat!'),\n",
    "    (3, 1, 'damu', 'Ahhh! Y de donde salio? Whatsapp?'),\n",
    "    (4, 2, 'martin', 'Sí! Cómo sabias?'),\n",
    "    (5, 1, 'damu', 'Adivine'),\n",
    "    (6, 3, 'luis', 'Hola!')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(chats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 'damu', 'Qué es esto?'),\n",
       " (2, 2, 'martin', 'Un chat!'),\n",
       " (3, 1, 'damu', 'Ahhh! Y de donde salio? Whatsapp?'),\n",
       " (4, 2, 'martin', 'Sí! Cómo sabias?'),\n",
       " (5, 1, 'damu', 'Adivine'),\n",
       " (6, 3, 'luis', 'Hola!')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: (x[1], x[3].count('?')))\\\n",
    "    .reduceByKey(lambda x, y: x+y)\\\n",
    "    .reduce(lambda x, y: x if (x[1] > y[1]) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-01 Parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UBER almacena en un cluster todos los datos sobre el movimiento y\n",
    "viajes de todos sus vehículos. Existe un proceso que nos devuelve un\n",
    "RDD llamado trip_summary con los siguientes campos: (driver_id,\n",
    "car_id, trip_id, customer_id, date (YYYYMMDD), distance_traveled),\n",
    "Programar usando Py Spark un programa que nos indique cual fue el\n",
    "conductor con mayor promedio de distancia recorrida por viaje para\n",
    "Abril de 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trips = [\n",
    "    (1, 1, 1, 1, '20160101', 10),\n",
    "    (2, 2, 2, 2, '20160202', 20),\n",
    "    (1, 1, 3, 1, '20160402', 15),\n",
    "    (1, 1, 4, 3, '20160405', 20),\n",
    "    (2, 2, 5, 4, '20160410', 25),\n",
    "    (3, 3, 6, 3, '20160415', 15),\n",
    "    (2, 2, 7, 1, '20160420', 40),\n",
    "    (3, 3, 8, 2, '20160505', 80)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(trips);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 1, 1, '20160101', 10),\n",
       " (2, 2, 2, 2, '20160202', 20),\n",
       " (1, 1, 3, 1, '20160402', 15),\n",
       " (1, 1, 4, 3, '20160405', 20),\n",
       " (2, 2, 5, 4, '20160410', 25),\n",
       " (3, 3, 6, 3, '20160415', 15),\n",
       " (2, 2, 7, 1, '20160420', 40),\n",
       " (3, 3, 8, 2, '20160505', 80)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter(lambda x: (x[4] > '20160400') and (x[4] < '20160500'))\\\n",
    "    .map(lambda x: (x[0], (1, x[5])))\\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "    .map(lambda x: (x[0], x[1][1]/x[1][0]))\\\n",
    "    .reduce(lambda x, y: x if x[1] > y[1] else y)\n",
    "    \n",
    "# El último map no es necesario, agregado solo por claridad. \n",
    "# Sin el map lo único que cambia es la comparación que se hace en el reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-02 2do Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un telescopio registra automaticamente la luminosidad de distintas\n",
    "estrellas generando un RDD con registros de tipo (star_id,\n",
    "magnitude,spectral_type, timestamp). Queremos obtener un listado de\n",
    "estrellas que tienen el mismo tipo espectral e igual promedio de\n",
    "magnitud una vez redondeado el mismo a un decimal. El resultado\n",
    "debe ser una lista en donde cada elemento de la lista es una lista de ids\n",
    "de estrellas similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stars = [\n",
    "    (1, 5, 1, 1),\n",
    "    (2, 10, 1, 1),\n",
    "    (3, 6, 1, 1),\n",
    "    (4, 5.5, 2, 1),\n",
    "    (1, 6, 1, 2),\n",
    "    (2, 9, 1, 2),\n",
    "    (3, 5, 1, 2),\n",
    "    (1, 5.5, 1, 3),\n",
    "    (2, 11, 1, 3),\n",
    "    (3, 5.5, 1, 3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize(stars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5, 1, 1),\n",
       " (2, 10, 1, 1),\n",
       " (3, 6, 1, 1),\n",
       " (4, 5.5, 2, 1),\n",
       " (1, 6, 1, 2),\n",
       " (2, 9, 1, 2),\n",
       " (3, 5, 1, 2),\n",
       " (1, 5.5, 1, 3),\n",
       " (2, 11, 1, 3),\n",
       " (3, 5.5, 1, 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 5.5), [1, 3]), ((1, 10.0), [2]), ((2, 5.5), [4])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(lambda x: (x[0], (x[2], x[1], 1)))\\\n",
    "    .reduceByKey(lambda x, y: (x[0], x[1]+y[1], x[2]+y[2]))\\\n",
    "    .map(lambda x: ((x[1][0], x[1][1]/x[1][2]), x[0]))\\\n",
    "    .groupByKey()\\\n",
    "    .map(lambda x: (x[0], list(x[1])))\\\n",
    "    .collect()\n",
    "    \n",
    "# El último map es simplemente para convertir la sequencia de spark en una lista de python para poder verla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015-01 2do Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una colección de documentos (textos) almacenados en un cluster. Se quiere establecer un ranking de los patrones mas frecuentes para la aparición de letras en las palabras. Por ejemplo “sister”, “ejects” , “ninety” y “amazon” responden al patrón “abacde”. Programar en map-reduce un programa que genere como resultado un\n",
    "listado de tipo (patron, frecuencia) indicando cuántas veces aparece cada patrón en la colección de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'README',\n",
       " '#',\n",
       " 'Repositorio',\n",
       " 'creado',\n",
       " 'para',\n",
       " 'ir',\n",
       " 'subiendo',\n",
       " 'los',\n",
       " 'diferentes',\n",
       " 'algoritmos',\n",
       " 'que',\n",
       " 'vayamos',\n",
       " 'programando',\n",
       " 'y',\n",
       " 'para',\n",
       " 'la',\n",
       " 'documentacion',\n",
       " 'que',\n",
       " 'vayamos',\n",
       " 'recolectando',\n",
       " 'a',\n",
       " 'medida',\n",
       " 'que',\n",
       " 'avanzamos',\n",
       " 'con',\n",
       " 'el',\n",
       " 'tp',\n",
       " '###',\n",
       " 'Estructura',\n",
       " 'de',\n",
       " 'directorios',\n",
       " '###',\n",
       " '.',\n",
       " '+--',\n",
       " '_data',\n",
       " '+--',\n",
       " '_docs',\n",
       " '+--',\n",
       " '_src',\n",
       " '|',\n",
       " '+--',\n",
       " 'python',\n",
       " '|',\n",
       " '+--',\n",
       " 'notebooks',\n",
       " 'En',\n",
       " 'data',\n",
       " 'guardaremos',\n",
       " 'los',\n",
       " 'archivos',\n",
       " 'train.csv',\n",
       " 'y',\n",
       " 'test.csv',\n",
       " 'en',\n",
       " 'nuestro',\n",
       " 'local.',\n",
       " 'En',\n",
       " 'docs',\n",
       " 'todo',\n",
       " 'lo',\n",
       " 'referente',\n",
       " 'a',\n",
       " 'diseño,',\n",
       " 'informe,',\n",
       " 'graficos,',\n",
       " 'etc.',\n",
       " 'En',\n",
       " 'src',\n",
       " 'vamos',\n",
       " 'ir',\n",
       " 'colocando',\n",
       " 'codigo',\n",
       " 'que',\n",
       " 'nos',\n",
       " 'ayude',\n",
       " 'a',\n",
       " 'resolver',\n",
       " 'el',\n",
       " 'TP.',\n",
       " 'Supuse',\n",
       " 'clasificarlo',\n",
       " 'por',\n",
       " 'lenguajes',\n",
       " 'por',\n",
       " 'si',\n",
       " 'tenemos',\n",
       " 'que',\n",
       " 'utilizar',\n",
       " 'varios.',\n",
       " 'En',\n",
       " 'la',\n",
       " 'carpeta',\n",
       " 'notebooks',\n",
       " 'se',\n",
       " 'tiene',\n",
       " 'que',\n",
       " 'crear',\n",
       " '(si',\n",
       " 'no',\n",
       " 'esta',\n",
       " 'subida',\n",
       " 'en',\n",
       " 'el',\n",
       " 'git)',\n",
       " 'una',\n",
       " 'carpeta',\n",
       " 'data',\n",
       " 'donde',\n",
       " 'se',\n",
       " 'guardará',\n",
       " 'el',\n",
       " 'train.csv',\n",
       " 'y',\n",
       " 'data.csv',\n",
       " 'para',\n",
       " 'ejecutar',\n",
       " 'los',\n",
       " 'notebooks',\n",
       " 'de',\n",
       " 'Jupyter.',\n",
       " '###',\n",
       " 'NO',\n",
       " 'AGREGAR',\n",
       " 'COSAS',\n",
       " 'INECESARIAS',\n",
       " 'AL',\n",
       " 'REPOSITORIO',\n",
       " 'REMOTO',\n",
       " '-',\n",
       " 'PREGUNTAR',\n",
       " 'ANTES',\n",
       " '###']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile = sc.textFile(\"README.md\")\n",
    "words = textFile.flatMap(lambda line: line.split())\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 85),\n",
       " ('ab', 58),\n",
       " ('abcd', 39),\n",
       " ('abcde', 37),\n",
       " ('abcdef', 16),\n",
       " ('abcdefg', 16),\n",
       " ('a', 12),\n",
       " ('aa', 8),\n",
       " ('abcdefgh', 8),\n",
       " ('abcdde', 6)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pattern(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    pat = ''\n",
    "    found = ''\n",
    "    for letter in word:\n",
    "        if letter in found:\n",
    "            pat += pat[found.index(letter)]\n",
    "        else:\n",
    "            found += letter\n",
    "            if len(found) > len(letters):\n",
    "                pat += '?'\n",
    "            else:\n",
    "                pat += letters[len(found)-1]\n",
    "    return pat\n",
    "   \n",
    "words.map(lambda x: (pattern(x), 1))\\\n",
    "     .reduceByKey(lambda x, y: x+y)\\\n",
    "     .takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2015-2C Parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se tiene un RDD con las coordenadas de rectángulos de la forma\n",
    "(x1,x2,y1,y2). Se pide programar en PySpark un programa que\n",
    "encuentre el rectángulo de superficie mínima que contiene al punto\n",
    "(w,z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rectangulos = [\n",
    "    (1.2, 3.1, 0.9, 3),\n",
    "    (1.2, -1.2, 5, 8),\n",
    "    (-4, -1.2, 0.8, 3.7),\n",
    "    (-4, -1.2, -3, -9.7),\n",
    "    (4, 1.2, -3, 3.7),\n",
    "    (1.2, 2.1, 0.9, 3)\n",
    "]\n",
    "point = (2,1)\n",
    "\n",
    "def isInsideOfRectangle(rectangle,point):\n",
    "    isInside = False\n",
    "    if(rectangle[0] > rectangle[1]):\n",
    "        isInside = (point[0]<=rectangle[0] and point[0]>=rectangle[1])\n",
    "    else:\n",
    "        isInside = (point[0]<=rectangle[1] and point[0]>=rectangle[0])\n",
    "        \n",
    "    if(rectangle[2] > rectangle[3]):\n",
    "        isInside = (point[1]<=rectangle[2] and point[1]>=rectangle[3])\n",
    "    else:\n",
    "        isInside = (point[1]<=rectangle[3] and point[1]>=rectangle[2])\n",
    "        \n",
    "    return isInside\n",
    "\n",
    "def surface(rectangle):\n",
    "    x = abs(rectangle[0] - rectangle[1])\n",
    "    y = abs(rectangle[2] - rectangle[3])\n",
    "    return x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2, 2.1, 0.9, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(rectangulos)\n",
    "\n",
    "rdd.filter(lambda x: isInsideOfRectangle(x,point))\\\n",
    "    .reduce(lambda x,y: x if surface(x)<surface(y) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016-2C 1er Recuperatorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tiene un RDD con información meteorológica de la forma (STATION_ID, YEAR, MONTH, DAY, HOUR, MINUTE, TEMPERATURE). Calcule primero la temperatura promedio para cada estación y día del año. Luego queremos encontrar cuál es la estación que ha sufrido una mayor variación absoluta de temperatura promedio entre un día y otro y cuál fue esa variación. Por ejemplo station_id = 43, variación = 43 grados. No es necesario indicar en qué fecha. Esto quiere decir que entre un día y otro el promedio de temperatura cambió en 18 grados (hacia arriba o abajo) para la estación 43."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "data = [\n",
    "    (1,2016,9,19,15,30,16),\n",
    "    (1,2016,9,19,16,30,15),\n",
    "    (1,2016,9,20,16,30,15),\n",
    "    (1,2016,9,21,16,30,13),\n",
    "    (1,2016,9,21,12,30,12),\n",
    "    (2,2016,9,19,15,30,16),\n",
    "    (2,2016,9,19,16,30,15),\n",
    "    (2,2016,9,20,16,30,15),\n",
    "    (2,2016,9,21,16,30,13),\n",
    "    (2,2016,9,21,12,30,12),\n",
    "    (4,2016,9,21,16,30,13),\n",
    "    (4,2016,9,21,12,30,12),\n",
    "]\n",
    "\n",
    "def diferenceDays(x,y):\n",
    "    a = str(x[0])+'-'+str(x[1])+'-'+str(x[2])\n",
    "    b = str(y[0])+'-'+str(y[1])+'-'+str(y[2])\n",
    "    \n",
    "    start = datetime.datetime.strptime(a, '%Y-%m-%d')\n",
    "    ends = datetime.datetime.strptime(b, '%Y-%m-%d')\n",
    "    \n",
    "    diff = relativedelta(start, ends)\n",
    "    \n",
    "    return diff.days\n",
    "    \n",
    "def getMaxVariationOfTemperature(arr):\n",
    "    prev = arr[0]\n",
    "    maxVariation = 0\n",
    "    for x in arr:\n",
    "        if(diferenceDays(prev,x)):\n",
    "            variation = abs(prev[3]-x[3])\n",
    "            if(variation>maxVariation):\n",
    "                maxVariation = variation\n",
    "    return maxVariation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2016, 9, 20, 15.0)),\n",
       " (2, (2016, 9, 21, 12.5)),\n",
       " (1, (2016, 9, 19, 15.5)),\n",
       " (2, (2016, 9, 19, 15.5)),\n",
       " (4, (2016, 9, 21, 12.5)),\n",
       " (2, (2016, 9, 20, 15.0)),\n",
       " (1, (2016, 9, 21, 12.5))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(data)\n",
    "\n",
    "rdd = rdd.map(lambda x: ((x[0],x[1],x[2],x[3]),(x[6],1)))\\\n",
    "    .reduceByKey(lambda x,y: (x[0]+y[0],x[1]+y[1]))\\\n",
    "    .map(lambda x: (x[0][0],(x[0][1],x[0][2],x[0][3],x[1][0]/x[1][1])))\n",
    "    \n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3.0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.sortBy(lambda x: (x[1][1],x[1][2],x[1][3]))\\\n",
    "    .groupByKey().map(lambda x: (x[0],list(x[1])))\\\n",
    "    .map(lambda x: (x[0],getMaxVariationOfTemperature(x[1])))\\\n",
    "    .reduce(lambda x,y: x if x[1]>y[1] else y)\n",
    "    #.reduceByKey(lambda x,y: abs(x[1][3]-y[1][3]) if ((y[1][2]-x[1][2]) == 1) else 0).take(1)\\\n",
    "    #.reduce(lambda x,y: x if x[1]>y[1] else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
